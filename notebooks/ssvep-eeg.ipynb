{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":98188,"databundleVersionId":12673416,"sourceType":"competition"},{"sourceId":476134,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":382930,"modelId":402385}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Notebook is my try to detect labels according to SSVEP Signals","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, random_split\nimport torch.optim as optim\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:31.106555Z","iopub.execute_input":"2025-07-20T15:34:31.107138Z","iopub.status.idle":"2025-07-20T15:34:31.111424Z","shell.execute_reply.started":"2025-07-20T15:34:31.107115Z","shell.execute_reply":"2025-07-20T15:34:31.110812Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"!ls /kaggle/input/mtcaic3\nbase_path = \"/kaggle/input/mtcaic3/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:31.122295Z","iopub.execute_input":"2025-07-20T15:34:31.122532Z","iopub.status.idle":"2025-07-20T15:34:31.289103Z","shell.execute_reply.started":"2025-07-20T15:34:31.122513Z","shell.execute_reply":"2025-07-20T15:34:31.288324Z"}},"outputs":[{"name":"stdout","text":"MI\t   sample_submission.csv  test.csv   validation.csv\nREADME.md  SSVEP\t\t  train.csv\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Here I load the data as pandas dataframe to analyze the data and to get the correlation between different features","metadata":{}},{"cell_type":"code","source":"def load_trial_data(row, base_path='.'):\n    id_num = row['id']\n    if id_num <= 4800:\n        dataset = 'train'\n    elif id_num <= 4900:\n        dataset = 'validation'\n    else:\n        dataset = 'test'\n\n    eeg_path = f\"{base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n    eeg_data = pd.read_csv(eeg_path)\n\n    trial_num = int(row['trial'])\n    samples_per_trial = 1750 if row['task'] == 'SSVEP' else 2250\n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial - 1\n    return eeg_data.iloc[start_idx:end_idx + 1]\n\ndef load_all_trials_df(df, base_path, start, end):\n    trials = []\n    for i in range(start, end):\n        row = df.iloc[i]\n        trial_df = load_trial_data(row, base_path)\n        trials.append(trial_df)\n\n    return trials\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:31.290928Z","iopub.execute_input":"2025-07-20T15:34:31.291157Z","iopub.status.idle":"2025-07-20T15:34:31.297811Z","shell.execute_reply.started":"2025-07-20T15:34:31.291135Z","shell.execute_reply":"2025-07-20T15:34:31.297204Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\nvalidation_df = pd.read_csv(os.path.join(base_path, 'validation.csv'))\ntest_df = pd.read_csv(os.path.join(base_path, 'test.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:31.298598Z","iopub.execute_input":"2025-07-20T15:34:31.298904Z","iopub.status.idle":"2025-07-20T15:34:31.331718Z","shell.execute_reply.started":"2025-07-20T15:34:31.298839Z","shell.execute_reply":"2025-07-20T15:34:31.331128Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"train_x = load_all_trials_df(train_df, base_path,2400,4800)\nval_x = load_all_trials_df(validation_df, base_path,50,100)\ntest_x = load_all_trials_df(test_df, base_path,50,100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:50:05.888258Z","iopub.execute_input":"2025-07-20T20:50:05.888552Z","iopub.status.idle":"2025-07-20T20:52:35.039678Z","shell.execute_reply.started":"2025-07-20T20:50:05.888533Z","shell.execute_reply":"2025-07-20T20:52:35.039089Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"\n\n# Assume train_x is a list of DataFrames\nnum_samples = len(train_x)\ncolumns = train_x[0].columns\n\n# Initialize correlation table with zeros\ncorrelation_table = pd.DataFrame(0.0, index=columns, columns=columns)\n\n# Accumulate correlations\nfor df in train_x:\n    for col1 in columns:\n        for col2 in columns:\n            corr = df[col1].corr(df[col2])\n            if pd.notnull(corr):\n                correlation_table.loc[col1, col2] += corr\n\n# Average over all samples\ncorrelation_table /= num_samples\n\n# Round and print\nprint(correlation_table.round(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:52:35.040921Z","iopub.execute_input":"2025-07-20T20:52:35.041470Z","iopub.status.idle":"2025-07-20T20:56:12.627730Z","shell.execute_reply.started":"2025-07-20T20:52:35.041442Z","shell.execute_reply":"2025-07-20T20:56:12.626917Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/usr/local/lib/python3.11/dist-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n","output_type":"stream"},{"name":"stdout","text":"             Time     FZ     C3     CZ     C4     PZ    PO7     OZ    PO8  \\\nTime        1.000  0.011 -0.021 -0.050 -0.080 -0.025  0.028 -0.014 -0.008   \nFZ          0.011  1.000  0.464  0.588  0.550  0.372  0.202  0.234  0.253   \nC3         -0.021  0.464  1.000  0.739  0.610  0.665  0.757  0.713  0.672   \nCZ         -0.050  0.588  0.739  1.000  0.867  0.812  0.640  0.715  0.731   \nC4         -0.080  0.550  0.610  0.867  1.000  0.782  0.590  0.680  0.741   \nPZ         -0.025  0.372  0.665  0.812  0.782  1.000  0.742  0.845  0.869   \nPO7         0.028  0.202  0.757  0.640  0.590  0.742  1.000  0.824  0.811   \nOZ         -0.014  0.234  0.713  0.715  0.680  0.845  0.824  1.000  0.890   \nPO8        -0.008  0.253  0.672  0.731  0.741  0.869  0.811  0.890  1.000   \nAccX       -0.000 -0.000 -0.001 -0.000 -0.002  0.002  0.004  0.002  0.001   \nAccY        0.038  0.001 -0.002 -0.003 -0.003  0.004  0.004  0.001 -0.000   \nAccZ        0.062  0.003 -0.006 -0.002 -0.003  0.003  0.010  0.002  0.002   \nGyro1       0.012  0.002 -0.001  0.000 -0.002  0.001 -0.002 -0.002 -0.001   \nGyro2       0.002  0.002  0.002  0.003  0.002  0.001  0.000 -0.002 -0.002   \nGyro3       0.003  0.003  0.005  0.004  0.004  0.005  0.007  0.004  0.005   \nBattery    -0.003  0.000  0.001  0.001  0.001  0.000 -0.000  0.000  0.000   \nCounter     0.999  0.011 -0.021 -0.050 -0.080 -0.025  0.029 -0.014 -0.008   \nValidation -0.003  0.001  0.001  0.000  0.000  0.001  0.001  0.001  0.001   \n\n             AccX   AccY   AccZ  Gyro1  Gyro2  Gyro3  Battery  Counter  \\\nTime       -0.000  0.038  0.062  0.012  0.002  0.003   -0.003    0.999   \nFZ         -0.000  0.001  0.003  0.002  0.002  0.003    0.000    0.011   \nC3         -0.001 -0.002 -0.006 -0.001  0.002  0.005    0.001   -0.021   \nCZ         -0.000 -0.003 -0.002  0.000  0.003  0.004    0.001   -0.050   \nC4         -0.002 -0.003 -0.003 -0.002  0.002  0.004    0.001   -0.080   \nPZ          0.002  0.004  0.003  0.001  0.001  0.005    0.000   -0.025   \nPO7         0.004  0.004  0.010 -0.002  0.000  0.007   -0.000    0.029   \nOZ          0.002  0.001  0.002 -0.002 -0.002  0.004    0.000   -0.014   \nPO8         0.001 -0.000  0.002 -0.001 -0.002  0.005    0.000   -0.008   \nAccX        1.000 -0.115  0.031 -0.018  0.055  0.138   -0.000   -0.000   \nAccY       -0.115  1.000  0.248 -0.024 -0.071 -0.144   -0.000    0.038   \nAccZ        0.031  0.248  1.000 -0.075  0.056  0.064   -0.000    0.061   \nGyro1      -0.018 -0.024 -0.075  1.000  0.036 -0.145   -0.000    0.012   \nGyro2       0.055 -0.071  0.056  0.036  1.000  0.068   -0.000    0.002   \nGyro3       0.138 -0.144  0.064 -0.145  0.068  1.000   -0.000    0.003   \nBattery    -0.000 -0.000 -0.000 -0.000 -0.000 -0.000    0.004   -0.003   \nCounter    -0.000  0.038  0.061  0.012  0.002  0.003   -0.003    1.000   \nValidation  0.000 -0.001 -0.001  0.001  0.000  0.000    0.000    0.001   \n\n            Validation  \nTime            -0.003  \nFZ               0.001  \nC3               0.001  \nCZ               0.000  \nC4               0.000  \nPZ               0.001  \nPO7              0.001  \nOZ               0.001  \nPO8              0.001  \nAccX             0.000  \nAccY            -0.001  \nAccZ            -0.001  \nGyro1            0.001  \nGyro2            0.000  \nGyro3            0.000  \nBattery          0.000  \nCounter          0.001  \nValidation       0.171  \n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"# --- 3. Your data loading functions ---\n\ndef load_trial_data(row, base_path='.'):\n    id_num = row['id']\n    if id_num <= 4800:\n        dataset = 'train'\n    elif id_num <= 4900:\n        dataset = 'validation'\n    else:\n        dataset = 'test'\n\n    eeg_path = f\"{base_path}/{row['task']}/{dataset}/{row['subject_id']}/{row['trial_session']}/EEGdata.csv\"\n    eeg_data = pd.read_csv(eeg_path)\n\n    trial_num = int(row['trial'])\n    samples_per_trial = 1750 if row['task'] == 'SSVEP' else 2250\n    start_idx = (trial_num - 1) * samples_per_trial\n    end_idx = start_idx + samples_per_trial - 1\n    return eeg_data.iloc[start_idx:end_idx + 1]\n\ndef load_all_trials(df, base_path, channels, start, end):\n    trials = []\n    for i in range(start, end):\n        row = df.iloc[i]\n        trial_df = load_trial_data(row, base_path)[channels].to_numpy()\n        trials.append(trial_df)\n\n    trials = np.stack(trials)\n    return trials\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:56:12.628614Z","iopub.execute_input":"2025-07-20T20:56:12.628884Z","iopub.status.idle":"2025-07-20T20:56:12.634788Z","shell.execute_reply.started":"2025-07-20T20:56:12.628857Z","shell.execute_reply":"2025-07-20T20:56:12.634117Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"### The Chosen features according to the correlation table","metadata":{}},{"cell_type":"code","source":"ssvep_channels = ['PO8', 'C4', 'FZ','C3','Time','AccX' ,  'AccY' ,  'AccZ' , 'Gyro1'  ,'Gyro2'  ,'Gyro3']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:56:12.636494Z","iopub.execute_input":"2025-07-20T20:56:12.636704Z","iopub.status.idle":"2025-07-20T20:56:12.652229Z","shell.execute_reply.started":"2025-07-20T20:56:12.636688Z","shell.execute_reply":"2025-07-20T20:56:12.651489Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"### Loading the data as numpy arrays","metadata":{}},{"cell_type":"code","source":"train_x = load_all_trials(train_df, base_path, ssvep_channels,2400,4800)\nval_x = load_all_trials(validation_df, base_path, ssvep_channels,50,100)\ntest_x = load_all_trials(test_df, base_path, ssvep_channels,50,100)\n\n# Since load_all_trials now returns a list, you can check the number of trials loaded\nprint(\"Number of trials in train_x:\", len(train_x))\nprint(\"Number of trials in val_x:\", len(val_x))\nprint(\"Number of trials in test_x:\", len(test_x))\n\n# You can also inspect the shape of the first few trials to see the different lengths\nprint(\"Shape of the first trial in train_x:\", train_x[0].shape)\nprint(\"Shape of the first trial in val_x:\", val_x[0].shape)\nprint(\"Shape of the first trial in test_x:\", test_x[0].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:56:12.653092Z","iopub.execute_input":"2025-07-20T20:56:12.653350Z","iopub.status.idle":"2025-07-20T20:58:25.998804Z","shell.execute_reply.started":"2025-07-20T20:56:12.653328Z","shell.execute_reply":"2025-07-20T20:58:25.998161Z"}},"outputs":[{"name":"stdout","text":"Number of trials in train_x: 2400\nNumber of trials in val_x: 50\nNumber of trials in test_x: 50\nShape of the first trial in train_x: (1750, 11)\nShape of the first trial in val_x: (1750, 11)\nShape of the first trial in test_x: (1750, 11)\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"train_labels=pd.read_csv(base_path+\"train.csv\")\ntrain_labels = train_labels[0:2400]\n\nval_labels=pd.read_csv(base_path+\"validation.csv\")\nval_labels = val_labels[0:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:25.999508Z","iopub.execute_input":"2025-07-20T20:58:25.999726Z","iopub.status.idle":"2025-07-20T20:58:26.011646Z","shell.execute_reply.started":"2025-07-20T20:58:25.999709Z","shell.execute_reply":"2025-07-20T20:58:26.010899Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"train_labels = train_labels.iloc[:,-1]\nval_labels = val_labels.iloc[:,-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:26.012592Z","iopub.execute_input":"2025-07-20T20:58:26.012912Z","iopub.status.idle":"2025-07-20T20:58:26.021805Z","shell.execute_reply.started":"2025-07-20T20:58:26.012837Z","shell.execute_reply":"2025-07-20T20:58:26.021079Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"label_to_index = {\n    \"Left\": 0,\n    \"Right\": 1,\n    \"Forward\":2,\n    \"Backward\":3\n}\nindex_to_label = {\n    0: \"Left\",\n    1: \"Right\",\n    2: \"Forward\",\n    3: \"Backward\"\n}\ntrain_labels = [label_to_index[label] for label in train_labels]\nval_labels = [label_to_index[label] for label in val_labels]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:26.022586Z","iopub.execute_input":"2025-07-20T20:58:26.022827Z","iopub.status.idle":"2025-07-20T20:58:26.037192Z","shell.execute_reply.started":"2025-07-20T20:58:26.022812Z","shell.execute_reply":"2025-07-20T20:58:26.036490Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"train_labels = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)  # shape: [N, 1]\nval_labels = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:26.037948Z","iopub.execute_input":"2025-07-20T20:58:26.038219Z","iopub.status.idle":"2025-07-20T20:58:26.052522Z","shell.execute_reply.started":"2025-07-20T20:58:26.038204Z","shell.execute_reply":"2025-07-20T20:58:26.051978Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"train_x = torch.from_numpy(train_x)\nval_x = torch.from_numpy(val_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:26.054144Z","iopub.execute_input":"2025-07-20T20:58:26.054352Z","iopub.status.idle":"2025-07-20T20:58:26.066378Z","shell.execute_reply.started":"2025-07-20T20:58:26.054336Z","shell.execute_reply":"2025-07-20T20:58:26.065607Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"train_x.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:58:26.066980Z","iopub.execute_input":"2025-07-20T20:58:26.067166Z","iopub.status.idle":"2025-07-20T20:58:26.081818Z","shell.execute_reply.started":"2025-07-20T20:58:26.067152Z","shell.execute_reply":"2025-07-20T20:58:26.081089Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"torch.Size([2400, 1750, 11])"},"metadata":{}}],"execution_count":76},{"cell_type":"markdown","source":"### Constructing the model","metadata":{}},{"cell_type":"code","source":"class TransformerClassifier(nn.Module):\n    def __init__(self, input_dim=11, d_model=32, num_heads=4, num_layers=3, num_classes=4, dropout=0.1):\n        super(TransformerClassifier, self).__init__()\n\n        self.input_bn = nn.BatchNorm1d(input_dim)\n\n        # Linear projection of input features to d_model\n        self.input_proj = nn.Linear(input_dim, d_model)\n\n        # Positional encoding: learnable or sinusoidal\n        self.pos_encoding = nn.Parameter(torch.randn(1, 1750, d_model))  # Assuming max length 1750\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.output_bn = nn.BatchNorm1d(d_model)\n        self.fc = nn.Linear(d_model, 4)\n\n    def forward(self, x):\n        # x: [B, T, F] → BatchNorm\n        x = self.input_bn(x.permute(0, 2, 1)).permute(0, 2, 1)  # [B, T, F]\n    \n        x = self.input_proj(x)  # [B, T, d_model]\n        x = x + self.pos_encoding[:, :x.size(1), :]  # Add positional encoding\n    \n        out = self.transformer(x)  # [B, T, d_model]\n        pooled = out[:, -1, :]  # Use last token's representation\n    \n        pooled = self.output_bn(pooled)  # BN before final layer\n        logits = self.fc(pooled)  # [B, 1]\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:28:16.703407Z","iopub.execute_input":"2025-07-20T16:28:16.704115Z","iopub.status.idle":"2025-07-20T16:28:16.710422Z","shell.execute_reply.started":"2025-07-20T16:28:16.704090Z","shell.execute_reply":"2025-07-20T16:28:16.709666Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class SequenceDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Return input sequence and label\n        return self.data[idx], self.labels[idx]\n\n\ntrain_dataset = SequenceDataset(train_x, train_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataset = SequenceDataset(val_x, val_labels)\nval_loader=DataLoader(val_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:35:30.916532Z","iopub.execute_input":"2025-07-20T15:35:30.916827Z","iopub.status.idle":"2025-07-20T15:35:30.922593Z","shell.execute_reply.started":"2025-07-20T15:35:30.916806Z","shell.execute_reply":"2025-07-20T15:35:30.921981Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model = TransformerClassifier()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T17:09:05.026033Z","iopub.execute_input":"2025-07-20T17:09:05.026281Z","iopub.status.idle":"2025-07-20T17:09:05.037086Z","shell.execute_reply.started":"2025-07-20T17:09:05.026263Z","shell.execute_reply":"2025-07-20T17:09:05.036382Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"\ndef evaluate(model, dataloader, device='cuda'):\n    model.eval()\n    correct = total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for x_batch, y_batch in dataloader:\n            x_batch = x_batch.to(device, torch.float32)\n            y_batch = y_batch.to(device).long().view(-1)  # Integer class labels\n\n            logits = model(x_batch)              # Shape: [B, num_classes]\n            preds = torch.argmax(logits, dim=1)  # Shape: [B]\n\n            correct += (preds == y_batch).sum().item()\n            total += y_batch.size(0)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y_batch.cpu().numpy())\n\n    accuracy = 100.0 * correct / total\n    f1 = f1_score(all_labels, all_preds, average='weighted')  # or 'macro'/'micro' if needed\n\n    return accuracy, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:47:59.811633Z","iopub.execute_input":"2025-07-20T16:47:59.812304Z","iopub.status.idle":"2025-07-20T16:47:59.818281Z","shell.execute_reply.started":"2025-07-20T16:47:59.812281Z","shell.execute_reply":"2025-07-20T16:47:59.817541Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"max_acc=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T16:48:00.125383Z","iopub.execute_input":"2025-07-20T16:48:00.125600Z","iopub.status.idle":"2025-07-20T16:48:00.128956Z","shell.execute_reply.started":"2025-07-20T16:48:00.125584Z","shell.execute_reply":"2025-07-20T16:48:00.128220Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"\ndef train(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda'):\n    global max_acc, max_model\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for x_batch, y_batch in train_loader:\n            x_batch = x_batch.to(device, torch.float32)\n            y_batch = y_batch.to(device).long().view(-1)  # e.g., [32]\n\n              # BCE expects float [B, 1]\n\n            optimizer.zero_grad()\n            logits = model(x_batch)  # [B, 1]\n            loss = criterion(logits, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n\n        # Evaluate on train and val\n        #train_acc, train_f1 = evaluate(model, train_loader, device)\n        val_acc, val_f1 = evaluate(model, val_loader, device)\n        if val_acc > max_acc:\n            max_acc = val_acc\n            torch.save(model.state_dict(), \"best_model_ssvep.pt\")\n\n        print(\n            f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} | \"\n            #f\"Train Acc: {train_acc:.2f}% - F1: {train_f1:.2f} | \"\n            f\"Val Acc: {val_acc:.2f}% - F1: {val_f1:.2f}\"\n        )\n\nfor _ in range(10):\n    model = TransformerClassifier()\n    train(model,train_loader,val_loader,35)\n\nprint(max_acc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_best = TransformerClassifier()\nmodel_best.load_state_dict(torch.load(\"/kaggle/working/best_model_ssvep.pt\", map_location=\"cpu\"))\nmodel_best.to(\"cpu\")\nmodel_best.eval()  # optional but recommended during inference\n\ntest = torch.from_numpy(test_x).to(torch.float32)  # ensure float32\ntest = test.to(\"cpu\")  # ensure it's also on CPU\n\nlogits = model_best(test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:48:50.797438Z","iopub.execute_input":"2025-07-20T20:48:50.797957Z","iopub.status.idle":"2025-07-20T20:48:54.874508Z","shell.execute_reply.started":"2025-07-20T20:48:50.797933Z","shell.execute_reply":"2025-07-20T20:48:54.873739Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"### In this try I succesfully got 66 accuracy for a classification for 4 classes","metadata":{}},{"cell_type":"code","source":"model_best.to(\"cuda\")\nevaluate(model_best, val_loader, \"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T20:48:54.875660Z","iopub.execute_input":"2025-07-20T20:48:54.875925Z","iopub.status.idle":"2025-07-20T20:48:55.024271Z","shell.execute_reply.started":"2025-07-20T20:48:54.875906Z","shell.execute_reply":"2025-07-20T20:48:55.023226Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(66.0, 0.6590289677682578)"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}